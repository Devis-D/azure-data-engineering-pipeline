{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Bronze to Silver - Date Transformation\n", "\n", "This notebook reads raw Parquet files from the **Bronze** layer, applies date column transformations,\n", "and writes the cleaned data as **Delta Lake** tables to the **Silver** layer.\n", "\n", "**Transformation**: All date/datetime columns are standardized to `yyyy-MM-dd` format."]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Explore Bronze and Silver Containers"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# List files in the Bronze layer\n", "bronze_path = \"wasbs://bronze@intechdataproject.blob.core.windows.net/SalesLT/\"\n", "bronze_files = dbutils.fs.ls(bronze_path)\n", "\n", "for f in bronze_files:\n", "    print(f.name)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# List contents of the Silver layer\n", "silver_path = \"wasbs://silver@intechdataproject.blob.core.windows.net/\"\n", "silver_files = dbutils.fs.ls(silver_path)\n", "\n", "for f in silver_files:\n", "    print(f.name)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Test with a Single Table (Address)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Read a single Parquet file from Bronze\n", "df = spark.read.parquet(bronze_path + \"Address.parquet\")\n", "print(\"Row count:\", df.count())\n", "print(\"Columns:\", df.columns)\n", "display(df)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Import required functions\n", "from pyspark.sql.functions import col, to_date\n", "\n", "# Convert ModifiedDate column to a clean date format\n", "df = df.withColumn(\"ModifiedDate\", to_date(col(\"ModifiedDate\")))\n", "\n", "display(df)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. Transform All Tables (Bronze to Silver)\n", "\n", "Loop through every table in the Bronze layer:\n", "1. Read the Parquet file\n", "2. Find any columns containing \"Date\" or \"date\"\n", "3. Convert those columns to a clean date format\n", "4. Write to Silver as Delta Lake"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Get all table names from the Bronze layer\n", "table_names = []\n", "\n", "for file in dbutils.fs.ls(bronze_path):\n", "    name = file.name.replace(\".parquet\", \"\")\n", "    table_names.append(name)\n", "\n", "print(\"Tables found:\", table_names)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["from pyspark.sql.functions import col, to_date\n", "\n", "for table in table_names:\n", "    # Read from Bronze\n", "    input_path = bronze_path + table + \".parquet\"\n", "    df = spark.read.parquet(input_path)\n", "\n", "    # Transform: convert all Date columns to clean date format\n", "    for column_name in df.columns:\n", "        if \"Date\" in column_name or \"date\" in column_name:\n", "            df = df.withColumn(column_name, to_date(col(column_name)))\n", "\n", "    # Write to Silver as Delta\n", "    output_path = \"wasbs://silver@intechdataproject.blob.core.windows.net/SalesLT/\" + table + \"/\"\n", "    df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n", "\n", "    print(f\"Processed: {table} ({df.count()} rows)\")\n", "\n", "print(\"\\nBronze to Silver transformation complete!\")"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Verify: display the last processed table\n", "display(df)"]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
